\documentclass[pdftex,12pt,letter]{article}
\usepackage[margin=0.75in]{geometry}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{xspace}
\usepackage{cite}
\usepackage{url}
\usepackage[pdftex,pdfpagelabels,bookmarks,hyperindex,hyperfigures]{hyperref}


\title{A proposal to implement a modern architecture for the Conditions Database Systems in DUNE}
\date{\today}
\author{P.\,Laycock, M.\,Potekhin}


\begin{document}
\maketitle

\begin{abstract}
\noindent  In the past few years the HEP community has developed fresh understanding of
the use cases and challenges of the Conditions Database systems based on the experience
of the LHC and other projects. New and converging designs have been created and
implemented in experiments such as CMS, Belle II and others. In this short paper we consider the features
of these new architectures and potential advantages they can bring to DUNE. In conclusion,
we note that the Conditions Database expertise that exists within the BNL team of developers
can be leveraged to the benefit of the DUNE experiment.

%% \cite{docdb1086}, DocDB\ and
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Overview}
According to  the HSF document titled \textbf{``A Roadmap for HEP Software and Computing R\&D for the 2020s''} \cite{hsf_roadmap},
\begin{quote}
\textit{
Conditions data is defined as the non-event data required by data-processing software to correctly simulate,
digitise or reconstruct the raw detector event data. The non-event data discussed here consists mainly of
detector calibration and alignment information, with some additional data describing the detector configuration,
the machine parameters, as well as information from the detector control system.
}
\end{quote}
\noindent It is important to note that the conditions data comprises a considerable variety of information,
which may also come in different formats and in different volumes depending on the use case. In the following
we will use the abbreviated term \textbf{CDB } to refer to the conditions database both as a concept and as an
instance where appropriate.

An crucial feature of the operating environment of a CDB is a potentially very high access rate
(hence high load on the database service) due to a large
 number of processing jobs runnning simultaneously in the distrubuted environment and requesting the conditions
data. Use of some sort of efficient caching mechanism is mandatory for the system to be performant.


\section{Architecture Convergence}

HEP and NP experiments have been using a variety of CDB systems over the past few decades.
More recently, the LHC experiments encountered issues related to the complexity and
maintainability of their systems \cite{func_test}. To meet the challenges of the ever increasing scale
and complexity of data processing the LHC experiments such as ATLAS and CMS have converged on
a common CDB architecture, which is also now shared by the Belle II experiment at KEK \cite{b2cdb}.
The salient features of this approach are as follows:
\begin{itemize}

\item Application-agnostic design of the database table schemas, with a limited number of
carefully designed tables

\item Separation of the \textbf{metadata} and the \textbf{content}
(commonly termed \textit{payloads} as there can be numerous types and pieces of data corresponding
to a particular time period and operating parameters of the experiment)

\item The CDB interface based on the REST technology

\item The design element often referred to as ``Global Tag'', which serves to aggregate a variety
of data and map it to a single unique key to facilitate access

\end{itemize}


\noindent  It is important to note that  the metadata is queried first in order to get a reference to the
set of payloads that comprise the conditions data, then the conditions data is served to the requestor.

In case of CMS, the payloads are stored as binary objects within a database table, whereas in Belle II
the data are kept in files which are served to the requestor using a highly performant NGNIX server instance
\cite{nginx}. A Web service locates the reference to the data first (as URLs pointing to the relevant set of files)
and the application then obtaines these data via the NGNIX server.

This design provides ample opportunities for caching. The metadata service can be fronted by a number
of caching solutions. Same applies to the content server (i.e. the service delivering the payloads) and in
addition solution such as CVMFS can be put into place.

\section{Current technology used in DUNE}

The current ``default'' solution for the conditions data is the \textit{ConDB} system developed and maintened
at FNAL \cite{condb}. It is modeled after the \textit{``COOL''} design used in ATLAS which is now considered obsolete and
difficult to maintain. The data model in ConDB is based on the notion of ``channels'' such as individual detector
channels. The basic use case which inspired ths design was one that included calibration constants for the detector channels.
There is no way to include arbitrary elements of data (e.g. MC-derived parameters, constants, maps etc) except for
making ``fake'' channels and mapping data to those channels which may be crafty and/or unwieldy. Binary objects
required by applications (e.g. ROOT files containing calibrations constants) are difficult and/or suboptimal
to incorporate. Since all data in ConDB is stored within the database itself, there is potential lack of scalability
when it comes to the systems with very large channel counts such as DUNE Far and Near detectors.

The modern design used in CMS, Belle II and now actively pursued in the ATLAS R\&D is mostly free of these limitiations.

\section{The Proposal}

Currently the  Brookhaven National Laboratory has a pool of talent who possess a considerable actual experience
and expertise in the design, implementation and operation of CDBs in a number of experiments. The successful
migration of the mission-critical Belle II CDB services from PNNL to BNL and subsequent problem-free operation of that
database are a testament to that. New hires at BNL such as Paul Laycock have considerable prior expertise in running
CDBs for ATLAS and other experiments.

We propose to launch a collaboration-wide effort in DUNE involving BNL to pursue and eventually implement the updated
design for the DUNE Conditions Database to ensure it is performant, easy to maintain and essentially future-proof.



%% \begin{figure}[tbh]
%%   \centering
%%   \includegraphics[width=1.1\textwidth]{figures/prompt_queues_1.pdf}
%%   \caption{Categories of prompt processing jobs grouped in queues. In this example, the depths of queues are different for different job types.}
%%   \label{fig:queues1}
%% \end{figure}


%%\newpage
%%\appendix
%%\section{Glossary}
%%\label{sec:glossary}
%%\begin{description}
%%\item [foo] description
%%\end{description}


\clearpage
\begin{thebibliography}{1}

\bibitem{hsf_roadmap}
{\textit{A Roadmap for HEP Software and Computing R\&D for the 2020s}arXiv:1712.06982} \url{https://arxiv.org/abs/1712.06982}

\bibitem{func_test}
{\textit{Functional tests of a prototype for the CMS-ATLAS common non-event data handling framework}} \url{https://iopscience.iop.org/article/10.1088/1742-6596/898/4/042047}

\bibitem{b2cdb}
{\textit{Conditions Database for the Belle II Experiment}}

\bibitem{nginx}
{\url{https://www.nginx.com/}}

\bibitem{condb}
{\textit{Conditions Database at Fermilab}  \url{https://cdcvs.fnal.gov/redmine/projects/condb/wiki/Conditions_Database_at_Fermilab}}

\end{thebibliography}


\end{document}
